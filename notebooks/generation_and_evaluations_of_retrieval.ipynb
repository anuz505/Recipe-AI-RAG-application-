{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anujb\\Documents\\code\\ChefAssistAI\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "from groq import Groq\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "groq_client = Groq(\n",
    "  api_key=GROQ_API_KEY\n",
    ")\n",
    "client = groq_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean.csv')\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Doro Wat',\n",
       " 'country': 'Ethiopia',\n",
       " 'ingredients': 'Chicken, onions, garlic, ginger, berbere spice mix, niter kibbeh',\n",
       " 'instructions': 'In a pot, sauté onions, garlic, and ginger in niter kibbeh until soft. Add berbere spice mix, cook for a few minutes, then add chicken. Cook until the chicken is tender.',\n",
       " 'meal_type': 'Main',\n",
       " 'spice_level': 'High',\n",
       " 'cooking_time_(minutes)': 90,\n",
       " 'vegetarian': 'No',\n",
       " 'main_cooking_method': 'Stewing',\n",
       " 'serving_temperature': 'Hot',\n",
       " 'how_to_make': 'Start by preparing niter kibbeh, a spiced clarified butter. Then sauté onions, garlic, and ginger in the niter kibbeh. Add berbere spice mix and stir for a few minutes. Add chicken pieces, cover, and cook until the chicken is thoroughly cooked.',\n",
       " 'id': 'f2ff4980-fda2-4bfe-8a05-aca63be38345'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a user of our Chief assistant application.\n",
    "Formulate 5 questions this user might ask based on a provided exercise.\n",
    "Make the questions specific to this exercise.\n",
    "The record should contain the answer to the questions, and the questions should\n",
    "be complete and not too short. Use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "\n",
    "name: {name},\n",
    "ingredients: {ingredients},\n",
    "instructions: {instructions},\n",
    "meal_type: {meal_type},\n",
    "spice_level: {spice_level},\n",
    "cooking_time_(minutes): {cooking_time_(minutes)},\n",
    "vegetarian: {vegetarian},\n",
    "main_cooking_method: {main_cooking_method},\n",
    "serving_temperature: {serving_temperature},\n",
    "how_to_make: {how_to_make}\n",
    "\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks and don't add anything else:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(**documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"questions\": [\"What type of dish is Doro Wat and what is its spice level?\", \"How long does it take to cook Doro Wat and what is the main cooking method used?\", \"What ingredients are used to sauté the onions, garlic, and ginger in the recipe?\", \"Is Doro Wat suitable for vegetarians and what is the recommended serving temperature?\", \"What is the first step in making Doro Wat and what is added after sautéing the onions, garlic, and ginger?\"]}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['What type of dish is Doro Wat and what is its spice level?',\n",
       "  'How long does it take to cook Doro Wat and what is the main cooking method used?',\n",
       "  'What ingredients are used to sauté the onions, garlic, and ginger in the recipe?',\n",
       "  'Is Doro Wat suitable for vegetarians and what is the recommended serving temperature?',\n",
       "  'What is the first step in making Doro Wat and what is added after sautéing the onions, garlic, and ginger?']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    json_response = response.choices[0].message.content\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [05:50<00:00,  2.11s/it]\n",
      "100%|██████████| 166/166 [05:50<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for doc in tqdm(documents): \n",
    "    doc_id = doc['id']\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "\n",
    "    questions_raw = generate_questions(doc)\n",
    "    questions = json.loads(questions_raw)\n",
    "    results[doc_id] = questions['questions']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f2ff4980-fda2-4bfe-8a05-aca63be38345',\n",
       " 'What is the primary protein used in Doro Wat?')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('36d849e4-35c1-4848-84e2-987fd1b33b9d',\n",
       " 'At what temperature is Mitarashi Dango typically served')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the primary protein used in Doro Wat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>How long does it take to prepare Doro Wat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the role of niter kibbeh in the recipe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>Is Doro Wat suitable for a vegetarian diet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the recommended serving temperature fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "1  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "2  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "3  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "4  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "\n",
       "                                            question  \n",
       "0      What is the primary protein used in Doro Wat?  \n",
       "1         How long does it take to prepare Doro Wat?  \n",
       "2    What is the role of niter kibbeh in the recipe?  \n",
       "3        Is Doro Wat suitable for a vegetarian diet?  \n",
       "4  What is the recommended serving temperature fo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>Doro Wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78c8ff6c-af26-4012-a637-af925fda17f7</td>\n",
       "      <td>Injera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3437bb23-0c42-4548-9165-f31f586b968f</td>\n",
       "      <td>Sushi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ec101289-7120-4818-99ec-40345acf99b8</td>\n",
       "      <td>Tacos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8395a5cd-7bd2-45f7-a19d-c4a232d02fa4</td>\n",
       "      <td>Paella</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      name\n",
       "0  f2ff4980-fda2-4bfe-8a05-aca63be38345  Doro Wat\n",
       "1  78c8ff6c-af26-4012-a637-af925fda17f7    Injera\n",
       "2  3437bb23-0c42-4548-9165-f31f586b968f     Sushi\n",
       "3  ec101289-7120-4818-99ec-40345acf99b8     Tacos\n",
       "4  8395a5cd-7bd2-45f7-a19d-c4a232d02fa4    Paella"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"id\", \"name\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv('../data/ground-truth-retrieval.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the primary protein used in Doro Wat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>How long does it take to prepare Doro Wat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the role of niter kibbeh in the recipe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>Is Doro Wat suitable for a vegetarian diet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the recommended serving temperature fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "1  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "2  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "3  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "4  f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "\n",
       "                                            question  \n",
       "0      What is the primary protein used in Doro Wat?  \n",
       "1         How long does it take to prepare Doro Wat?  \n",
       "2    What is the role of niter kibbeh in the recipe?  \n",
       "3        Is Doro Wat suitable for a vegetarian diet?  \n",
       "4  What is the recommended serving temperature fo...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_question.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f2ff4980-fda2-4bfe-8a05-aca63be38345',\n",
       " 'question': 'What is the primary protein used in Doro Wat?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = df_question.to_dict(orient='records')\n",
    "ground_truth[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "index_name = 'chefrag'  # Changed to match your app configuration\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and uploading to Pinecone...\n",
      "Prepared 166 documents for embedding\n",
      "Sample metadata keys: ['name', 'country', 'ingredients', 'instructions', 'meal_type', 'spice_level', 'cooking_time_(minutes)', 'vegetarian', 'main_cooking_method', 'serving_temperature', 'how_to_make', 'id']\n",
      "Sample metadata values with None: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.12s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 166 documents to Pinecone!\n",
      "Final index stats:\n",
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n",
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and upload to Pinecone\n",
    "print(\"Creating embeddings and uploading to Pinecone...\")\n",
    "\n",
    "def clean_metadata(doc):\n",
    "    \"\"\"Clean metadata by handling NaN values and ensuring Pinecone compatibility\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in doc.items():\n",
    "        if pd.isna(value):\n",
    "            # Skip None/NaN values entirely - don't include them in metadata\n",
    "            continue\n",
    "        elif isinstance(value, (int, float)) and not pd.isna(value):\n",
    "            cleaned[key] = value\n",
    "        elif value is not None:\n",
    "            cleaned[key] = str(value)\n",
    "        # Skip None values entirely\n",
    "    return cleaned\n",
    "\n",
    "def clean_value_for_text(value):\n",
    "    \"\"\"Clean individual values for text representation\"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return \"\"\n",
    "    return str(value)\n",
    "\n",
    "# Prepare documents for embedding\n",
    "docs_to_embed = []\n",
    "for i, doc in enumerate(documents):\n",
    "    # Clean the document first\n",
    "    clean_doc = clean_metadata(doc)\n",
    "    \n",
    "    # Create a text representation of each document using cleaned values\n",
    "    doc_text = f\"\"\"\n",
    "    name: {clean_value_for_text(doc.get('name', ''))},\n",
    "    ingredients: {clean_value_for_text(doc.get('ingredients', ''))},\n",
    "    instructions: {clean_value_for_text(doc.get('instructions', ''))},\n",
    "    meal_type: {clean_value_for_text(doc.get('meal_type', ''))},\n",
    "    spice_level: {clean_value_for_text(doc.get('spice_level', ''))},\n",
    "    cooking_time_(minutes): {clean_value_for_text(doc.get('cooking_time_(minutes)', ''))},\n",
    "    vegetarian: {clean_value_for_text(doc.get('vegetarian', ''))},\n",
    "    main_cooking_method: {clean_value_for_text(doc.get('main_cooking_method', ''))},\n",
    "    serving_temperature: {clean_value_for_text(doc.get('serving_temperature', ''))},\n",
    "    how_to_make: {clean_value_for_text(doc.get('how_to_make', ''))}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    docs_to_embed.append({\n",
    "        'id': str(clean_doc.get('id', i)),\n",
    "        'text': doc_text,\n",
    "        'metadata': clean_doc  # Use cleaned metadata (no None values)\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(docs_to_embed)} documents for embedding\")\n",
    "\n",
    "# Let's check a sample to make sure no None values\n",
    "sample_metadata = docs_to_embed[0]['metadata']\n",
    "print(f\"Sample metadata keys: {list(sample_metadata.keys())}\")\n",
    "print(f\"Sample metadata values with None: {[k for k, v in sample_metadata.items() if v is None]}\")\n",
    "\n",
    "# Create embeddings in batches\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(docs_to_embed), batch_size)):\n",
    "    batch = docs_to_embed[i:i+batch_size]\n",
    "    \n",
    "    # Create embeddings for this batch\n",
    "    texts = [item['text'] for item in batch]\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    # Prepare vectors for Pinecone\n",
    "    vectors_to_upsert = []\n",
    "    for j, (embedding, item) in enumerate(zip(embeddings, batch)):\n",
    "        # Final cleanup: ensure no None values in the final metadata\n",
    "        final_metadata = {\n",
    "            'text': item['text'],\n",
    "            **{k: v for k, v in item['metadata'].items() if v is not None}\n",
    "        }\n",
    "        \n",
    "        vectors_to_upsert.append({\n",
    "            'id': item['id'],\n",
    "            'values': embedding.tolist(),\n",
    "            'metadata': final_metadata\n",
    "        })\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    index.upsert(vectors=vectors_to_upsert)\n",
    "    \n",
    "print(f\"Successfully uploaded {len(docs_to_embed)} documents to Pinecone!\")\n",
    "print(\"Final index stats:\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_documents(batch_size=10):\n",
    "    documents = []\n",
    "    total_vectors = index.describe_index_stats()['total_vector_count']\n",
    "    # print(f\"Total vectors in index: {total_vectors}\")\n",
    "    \n",
    "    random_vector = np.random.rand(384).tolist()  # Assuming 384 is the dimension of your vectors\n",
    "    \n",
    "    # Query all vectors\n",
    "    query_response = index.query(\n",
    "        vector=random_vector,\n",
    "        top_k=total_vectors,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # print(f\"Retrieved {len(query_response['matches'])} vectors\")\n",
    "    \n",
    "    for match in query_response['matches']:\n",
    "        # print(f\"Vector ID: {match.id}\")\n",
    "        # print(f\"Vector metadata: {match.metadata}\")\n",
    "        \n",
    "        if 'text' in match.metadata:\n",
    "            documents.append(match.metadata['text'])\n",
    "        else:\n",
    "            print(f\"Warning: 'text' not found in metadata for vector {match.id}\")\n",
    "    \n",
    "    # print(f\"Total documents retrieved: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def keyword_search(query, documents, top_k=15):\n",
    "    tokenized_corpus = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "    return [(idx, bm25_scores[idx]) for idx in top_n]\n",
    "\n",
    "def query_pinecone(query, top_k=5):\n",
    "    xq = model.encode(query).tolist()\n",
    "    xc = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    results = []\n",
    "    for match in xc.matches:\n",
    "        result = {\n",
    "            \"id\": match.id,\n",
    "            \"score\": match.score,\n",
    "            \"metadata\": match.metadata\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    # print(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hybrid_query_pinecone(query, documents, top_k=15, alpha=0.8):\n",
    "    # Vector search\n",
    "    xq = model.encode(query).tolist()\n",
    "    vector_results = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    # Keyword search\n",
    "    keyword_results = keyword_search(query, documents, top_k=top_k)\n",
    "    \n",
    "    # Combine results\n",
    "    combined_results = {}\n",
    "    for match in vector_results.matches:\n",
    "        combined_results[match.id] = {\n",
    "            \"id\": match.id,\n",
    "            \"vector_score\": match.score,\n",
    "            \"keyword_score\": 0,\n",
    "            \"metadata\": match.metadata\n",
    "        }\n",
    "    \n",
    "    for idx, score in keyword_results:\n",
    "        if idx in combined_results:\n",
    "            combined_results[idx][\"keyword_score\"] = score\n",
    "        else:\n",
    "            combined_results[idx] = {\n",
    "                \"id\": idx,\n",
    "                \"vector_score\": 0,\n",
    "                \"keyword_score\": score,\n",
    "                \"metadata\": None  # You might want to fetch metadata for these results\n",
    "            }\n",
    "    \n",
    "    # Calculate hybrid score\n",
    "    for result in combined_results.values():\n",
    "        result[\"hybrid_score\"] = alpha * result[\"vector_score\"] + (1 - alpha) * result[\"keyword_score\"]\n",
    "    \n",
    "    # Sort by hybrid score and return top results\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x[\"hybrid_score\"], reverse=True)[:top_k]\n",
    "    \n",
    "    return [result for result in sorted_results if result[\"hybrid_score\"] > 0.2]\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    count = 0\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        # print(doc_id)\n",
    "        # try:\n",
    "        #     results = search_function(q)\n",
    "        #     count += 1\n",
    "        # except:\n",
    "        #     pass\n",
    "        results = search_function(q)\n",
    "        count += 1\n",
    "        # print(results)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        # print(relevance)\n",
    "        relevance_total.append(relevance)\n",
    "        # break\n",
    "\n",
    "    # print(count)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 830/830 [05:24<00:00,  2.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9891566265060241, 'mrr': 0.9505823293172688}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: query_pinecone(q['question']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- {'hit_rate': 0.8880778588807786, 'mrr': 0.848337388483374}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all_documents: 166\n",
      "First 5 documents:\n",
      "['name: Chicken Tikka Masala,\\n    ingredients: Chicken, yogurt, tomatoes, onions, garlic, ginger, spices,\\n    instructions: Marinate chicken in yogurt and spices, grill. Cook in a sauce of tomatoes, onions, and spices.,\\n    meal_type: Main,\\n    spice_level: Medium,\\n    cooking_time_(minutes): 45,\\n    vegetarian: No,\\n    main_cooking_method: Grilling,\\n    serving_temperature: Hot,\\n    how_to_make: Marinate chicken in yogurt and spices, grill until charred, then simmer in a tomato-based sauce until cooked through.', 'name: Tiramisu,\\n    ingredients: Mascarpone, espresso, ladyfingers, cocoa powder,\\n    instructions: Layer mascarpone with espresso-dipped ladyfingers, chill and dust with cocoa powder.,\\n    meal_type: Dessert,\\n    spice_level: ,\\n    cooking_time_(minutes): 20,\\n    vegetarian: Yes,\\n    main_cooking_method: No-cook,\\n    serving_temperature: Cold,\\n    how_to_make: Whisk mascarpone with sugar, dip ladyfingers in espresso, layer with cream mixture, chill and dust with cocoa powder before serving.', 'name: Baba Ganoush,\\n    ingredients: Eggplant, tahini, garlic, lemon, olive oil,\\n    instructions: Roast eggplant, blend with tahini, garlic, lemon, and olive oil.,\\n    meal_type: Appetizer,\\n    spice_level: ,\\n    cooking_time_(minutes): 30,\\n    vegetarian: Yes,\\n    main_cooking_method: Roasting,\\n    serving_temperature: Room,\\n    how_to_make: Roast whole eggplant until charred, scoop out flesh, blend with tahini, garlic, lemon juice, and olive oil.', 'name: Tom Yum Soup,\\n    ingredients: Shrimp, lemongrass, galangal, lime leaves, chilies,\\n    instructions: In a pot, boil water with lemongrass, galangal, and lime leaves. Add chilies, shrimp, and simmer until cooked.,\\n    meal_type: Soup,\\n    spice_level: High,\\n    cooking_time_(minutes): 30,\\n    vegetarian: No,\\n    main_cooking_method: Boiling,\\n    serving_temperature: Hot,\\n    how_to_make: Boil broth with lemongrass, galangal, lime leaves, and chilies. Add shrimp and simmer until cooked.', 'name: Chicken Tikka,\\n    ingredients: Chicken, yogurt, spices, lemon juice, garlic,\\n    instructions: Marinate chicken in spiced yogurt, grill until charred.,\\n    meal_type: Main,\\n    spice_level: High,\\n    cooking_time_(minutes): 30,\\n    vegetarian: No,\\n    main_cooking_method: Grilling,\\n    serving_temperature: Hot,\\n    how_to_make: Mix chicken pieces with yogurt, spices, lemon juice, and garlic, marinate, grill until cooked and charred.']\n"
     ]
    }
   ],
   "source": [
    "all_documents = get_all_documents()\n",
    "print(f\"Length of all_documents: {len(all_documents)}\")\n",
    "print(\"First 5 documents:\")\n",
    "print(all_documents[:5])  # Print the first 5 documents to see their content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 830/830 [05:21<00:00,  2.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.4819277108433735, 'mrr': 0.09494723482675312}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: hybrid_query_pinecone(q['question'], all_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks and make sure it is json parsable and nothing else is added\n",
    "to the below structure:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the primary protein used in Doro Wat?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the primary protein used in Doro Wat?\n",
      "Answer: The primary protein used in Doro Wat is not specified in the given information, as Doro Wat is not among the listed dishes. However, based on general knowledge, Doro Wat is a traditional Ethiopian dish, and its primary protein is usually chicken.\n"
     ]
    }
   ],
   "source": [
    "def format_dish_info(dish):\n",
    "    return \"\\n\".join([f\"{key}: {value}\" for key, value in dish['metadata'].items() if value])\n",
    "\n",
    "def qa_function(question):\n",
    "    # Query Pinecone\n",
    "    results = query_pinecone(question)\n",
    "    # print(results)\n",
    "    if not results:\n",
    "        return \"I'm sorry, I couldn't find any relevant information to answer your question.\"\n",
    "    \n",
    "    # Format the dish information\n",
    "    all_dish_info = \"\\n\\n\".join([format_dish_info(dish) for dish in results])\n",
    "\n",
    "    # print(all_dish_info)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following information about a dish, please answer the question: {question}\n",
    "\n",
    "    Dish information:\n",
    "    {all_dish_info}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use Groq to generate an answer\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def llm(prompt, model=\"llama-3.3-70b-versatile\"):\n",
    "    client = Groq(\n",
    "        api_key=GROQ_API_KEY\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Test the QA function\n",
    "question = ground_truth[0][\"question\"]\n",
    "answer = qa_function(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a RAG system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: What is the primary protein used in Doro Wat?\n",
      "Generated Answer: The primary protein used in Doro Wat is not specified in the given information, as Doro Wat is not among the listed dishes. However, based on general knowledge, Doro Wat is a traditional Ethiopian dish, and its primary protein is usually chicken.\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks and make sure it is json parsable and nothing else is added\n",
      "to the below structure:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(question=question, answer_llm=answer)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_question.sample(n=200, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>d306030a-75d6-4421-8c87-788b35610eb3</td>\n",
       "      <td>How long does it take to cook Tarte Tatin in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>4f147bad-7ace-4951-a05e-d4e8fb1f3ebd</td>\n",
       "      <td>Is Pumpkin Soup suitable for a vegetarian diet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>8f03b61d-e6e3-490e-92da-04bc8d207b18</td>\n",
       "      <td>Do Croquettes contain any meat ingredients?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>033d2093-952e-4b56-a11b-e990344ccefe</td>\n",
       "      <td>How would you describe the texture and appeara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>dbcb585d-c83f-4b93-b925-9b8e641ded80</td>\n",
       "      <td>What is the recommended serving temperature fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f2ff4980-fda2-4bfe-8a05-aca63be38345</td>\n",
       "      <td>What is the primary protein used in Doro Wat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>8d1f94b2-6458-4fb2-a38a-ab67afd6f80a</td>\n",
       "      <td>What is the first step in preparing the Risotto?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>726da803-6b23-470e-b469-5ff6a1a855f6</td>\n",
       "      <td>How long does it take to cook Hainanese Chicke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1c7876c7-05d5-46e0-a12a-1ef72b241012</td>\n",
       "      <td>Does Borscht have a high spice level?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>e8d90b0c-cc32-4f10-9a7a-c3af3bac39ca</td>\n",
       "      <td>What type of meal is Clam Chowder and what is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "786  d306030a-75d6-4421-8c87-788b35610eb3   \n",
       "522  4f147bad-7ace-4951-a05e-d4e8fb1f3ebd   \n",
       "811  8f03b61d-e6e3-490e-92da-04bc8d207b18   \n",
       "579  033d2093-952e-4b56-a11b-e990344ccefe   \n",
       "573  dbcb585d-c83f-4b93-b925-9b8e641ded80   \n",
       "..                                    ...   \n",
       "0    f2ff4980-fda2-4bfe-8a05-aca63be38345   \n",
       "344  8d1f94b2-6458-4fb2-a38a-ab67afd6f80a   \n",
       "286  726da803-6b23-470e-b469-5ff6a1a855f6   \n",
       "74   1c7876c7-05d5-46e0-a12a-1ef72b241012   \n",
       "160  e8d90b0c-cc32-4f10-9a7a-c3af3bac39ca   \n",
       "\n",
       "                                              question  \n",
       "786  How long does it take to cook Tarte Tatin in t...  \n",
       "522    Is Pumpkin Soup suitable for a vegetarian diet?  \n",
       "811        Do Croquettes contain any meat ingredients?  \n",
       "579  How would you describe the texture and appeara...  \n",
       "573  What is the recommended serving temperature fo...  \n",
       "..                                                 ...  \n",
       "0        What is the primary protein used in Doro Wat?  \n",
       "344   What is the first step in preparing the Risotto?  \n",
       "286  How long does it take to cook Hainanese Chicke...  \n",
       "74               Does Borscht have a high spice level?  \n",
       "160  What type of meal is Clam Chowder and what is ...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_sample.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [02:41<17:57,  6.20s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4c06yfhfn1r8137h42k3yet` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99443, Requested 1196. Please try again in 9m11.865s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m tqdm(sample):\n\u001b[32m      4\u001b[39m     question = record[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     answer_llm = \u001b[43mqa_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      7\u001b[39m     prompt = prompt2_template.format(\n\u001b[32m      8\u001b[39m         question=question,\n\u001b[32m      9\u001b[39m         answer_llm=answer_llm\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m     evaluation = llm(prompt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mqa_function\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m     17\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33mBased on the following information about a dish, please answer the question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33mAnswer:\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Use Groq to generate an answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mllm\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     32\u001b[39m     client = Groq(\n\u001b[32m     33\u001b[39m         api_key=GROQ_API_KEY\n\u001b[32m     34\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     chat_completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# print(chat_completion.choices[0].message.content)\u001b[39;00m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anujb\\Documents\\code\\ChefAssistAI\\myvenv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:378\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    186\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    234\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    235\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    376\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anujb\\Documents\\code\\ChefAssistAI\\myvenv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anujb\\Documents\\code\\ChefAssistAI\\myvenv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4c06yfhfn1r8137h42k3yet` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99443, Requested 1196. Please try again in 9m11.865s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = qa_function(question) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "        evaluations.append((record, answer_llm, evaluation))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_eval = pd.DataFrame(\u001b[43mevaluations\u001b[49m, columns=[\u001b[33m'\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mevaluation\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m df_eval[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m] = df_eval.record.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: d[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m df_eval[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m] = df_eval.record.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: d[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluations' is not defined"
     ]
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[df_eval[\"relevance\"] == \"NON_RELEVANT\"][[\"question\", \"answer\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
